{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir('../')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from rouge_score import rouge_scorer\n",
    "from utils.config import load_config\n",
    "from ast import literal_eval as le\n",
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "from src.openai_evaluator import GPT4Accuracy, GPT4SingleValue\n",
    "from dotenv import load_dotenv; load_dotenv('.env')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config('config/climate.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRMSEScore(pred_values, fut_values):\n",
    "    y_pred = np.reshape(pred_values, -1)\n",
    "    y_true = np.reshape(fut_values, -1)\n",
    "    y_pred = np.array(y_pred, dtype=np.float64)\n",
    "    y_true = np.array(y_true, dtype=np.float64)\n",
    "    \n",
    "    return np.sqrt(np.mean(np.square(y_pred - y_true)))\n",
    "\n",
    "def getMeteorScore(outputs, pred_outputs):\n",
    "    scores = [meteor([word_tokenize(output)], word_tokenize(pred_output)) \n",
    "              for output, pred_output in tqdm(zip(outputs, pred_outputs), \n",
    "                                              total=len(outputs), \n",
    "                                              desc=\"Calculating METEOR scores\")]\n",
    "    mean_score = np.mean(scores)\n",
    "    \n",
    "    return mean_score\n",
    "\n",
    "cos_model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "def getCosineSimilarity(outputs, pred_outputs):\n",
    "    cos_sims = [cos_sim(x, y) \n",
    "                for x, y in tqdm(zip(cos_model.encode(outputs), cos_model.encode(pred_outputs)), \n",
    "                                 total=len(outputs), \n",
    "                                 desc=\"Calculating Cosine Similarities\")]\n",
    "    return np.mean(cos_sims)\n",
    "\n",
    "def getROUGEScore(outputs, pred_outputs):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for output, pred_output in zip(outputs, pred_outputs):\n",
    "        scores = scorer.score(output, pred_output)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    mean_rouge1 = np.mean(rouge1_scores)\n",
    "    mean_rouge2 = np.mean(rouge2_scores)\n",
    "    mean_rougeL = np.mean(rougeL_scores)\n",
    "    \n",
    "    return mean_rouge1, mean_rouge2, mean_rougeL\n",
    "\n",
    "def split_forecast_by_day(text, output_window):\n",
    "    days = re.split(r'(day_\\d+_date:)', text)\n",
    "    days = [days[i] + days[i+1] for i in range(1, len(days)-1, 2)]\n",
    "    days = [day.strip() for day in days][:output_window]\n",
    "    return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = GPT4Accuracy(description=\"multimodal_medical_bge_2\")\n",
    "evaluator = GPT4SingleValue(description=\"multimodal_climate_single\")\n",
    "all_windows = [1, 2, 3, 4, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_object_map = {}\n",
    "root_dir = \"results_climate/hybrid_stage_3\"\n",
    "\n",
    "for w in all_windows:\n",
    "    input_window=w\n",
    "    output_window=w\n",
    "\n",
    "    df = pd.read_csv(f\"{root_dir}/{input_window}_{output_window}/hybrid_results.csv\")\n",
    "\n",
    "    df['input_times'] = df['input_times'].apply(lambda x: np.fromstring(x.strip('[]'), sep=' ').tolist())\n",
    "    df['output_times'] = df['output_times'].apply(lambda x: np.fromstring(x.strip('[]'), sep=' ').tolist())\n",
    "    df['input_dates'] = df['input_dates'].apply(le)\n",
    "    df['output_dates'] = df['output_dates'].apply(le)\n",
    "    df['input_texts'] = df['input_texts'].apply(le)\n",
    "    df['output_texts'] = df['output_texts'].apply(le)\n",
    "    df['pred_times'] = df['pred_times'].apply(lambda x: np.fromstring(x.strip('[]'), sep=' ').tolist())\n",
    "\n",
    "    # format ground truth text\n",
    "    output_texts = []\n",
    "    for text, date in zip(df['output_texts'], df['output_dates']):\n",
    "        output_text = []\n",
    "        for i in range(output_window):\n",
    "            date_template = cfg['date_template'].format(index=input_window+1+i)\n",
    "            text_template = cfg['text_template'].format(index=input_window+1+i)\n",
    "            output_text.append(f\"{date_template}: {date[i]}\\n{text_template}: {text[i]}\")\n",
    "        output_texts.append('\\n'.join(output_text))\n",
    "\n",
    "    df['output_texts'] = output_texts\n",
    "\n",
    "\n",
    "    pred_texts = df['pred_texts'].apply(lambda x: split_forecast_by_day(x, w)).tolist()\n",
    "    output_texts = [split_forecast_by_day(text, w) for text in output_texts]\n",
    "    split_data = []\n",
    "    for pred, output in zip(pred_texts, output_texts):\n",
    "        for p, o in zip(pred, output):\n",
    "            split_data.append({'pred_text': p, 'output_text': o})\n",
    "\n",
    "    df_evaluation = pd.DataFrame(split_data)\n",
    "\n",
    "    rmse_score = getRMSEScore(np.stack(np.array(df['output_times'])), np.stack(np.array(df['pred_times'])))\n",
    "    cosine_score = getCosineSimilarity(df_evaluation['output_text'], df_evaluation['pred_text'])\n",
    "    meteor_score = getMeteorScore(df_evaluation['output_text'], df_evaluation['pred_text'])\n",
    "    rouge_1_score, rouge_2_score, rouge_n_score = getROUGEScore(df_evaluation['output_text'], df_evaluation['pred_text'])\n",
    "\n",
    "    print(f\"-------{w}----------\")\n",
    "    print(round(rmse_score, 3))\n",
    "    print(round(cosine_score, 3))\n",
    "    print(round(meteor_score, 3))\n",
    "    print(round(rouge_1_score, 3), round(rouge_2_score, 3), round(rouge_n_score, 3))\n",
    "\n",
    "    jsonl_path = f\"{root_dir}/{input_window}_{output_window}/accuracy_evaluator.jsonl\"\n",
    "    batch_object_id = evaluator.create_and_run_batch_job(df_evaluation, jsonl_path)\n",
    "    batch_object_map[w] = batch_object_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x.status, x.request_counts, x.id) for x in evaluator.client.batches.list().data[:len(all_windows)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_single_value(parsed_outputs):\n",
    "    gpt_scores = []\n",
    "    for output in parsed_outputs:\n",
    "        number = re.findall(r'\\d+', output)\n",
    "        if len(number) > 0:\n",
    "            gpt_score = float(number[0])\n",
    "        else:\n",
    "            gpt_score = 0\n",
    "        gpt_scores.append(gpt_score)\n",
    "\n",
    "    return np.mean(gpt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in all_windows:\n",
    "    output_path = f\"{root_dir}/{id}_{id}/accuracy_evaluator.txt\"\n",
    "    parsed_outputs = evaluator.check_status_and_parse(batch_object_map[id], output_path)\n",
    "\n",
    "    precision, recall, f1 = evaluator.calculate_metrics(parsed_outputs)\n",
    "    print(f'--------{id}----------')\n",
    "    print(round(precision, 3))\n",
    "    print(round(recall, 3))\n",
    "    print(round(f1, 3))\n",
    "\n",
    "    single_score = calculate_single_value(parsed_outputs)\n",
    "    print(round(single_score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
